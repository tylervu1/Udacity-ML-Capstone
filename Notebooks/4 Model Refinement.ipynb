{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Urban sounds using Deep Learning\n",
    "\n",
    "## 4 Model Refinement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve the preprocessed data from previous notebook\n",
    "\n",
    "%store -r x_train \n",
    "%store -r x_test \n",
    "%store -r y_train \n",
    "%store -r y_test \n",
    "%store -r yy \n",
    "%store -r le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model refinement\n",
    "\n",
    "In our inital attempt, we were able to achieve a Classification Accuracy score of: \n",
    "\n",
    "* Training data Accuracy:  92.3% \n",
    "* Testing data Accuracy:  87% \n",
    "\n",
    "We will now see if we can improve upon that score using a Convolutional Neural Network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction refinement \n",
    "\n",
    "In the prevous feature extraction stage, the MFCC vectors would vary in size for the different audio files (depending on the samples duration). \n",
    "\n",
    "However, CNNs require a fixed size for all inputs. To overcome this we will zero pad the output vectors to make them all the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_pad_len = 174\n",
    "\n",
    "def extract_features(file_name):\n",
    "   \n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    pad_width = max_pad_len - mfccs.shape[1]\n",
    "    mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylervu/anaconda3/lib/python3.11/site-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "/Users/tylervu/anaconda3/lib/python3.11/site-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "/Users/tylervu/anaconda3/lib/python3.11/site-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  8732  files\n"
     ]
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = '../UrbanSound8K/audio/'\n",
    "\n",
    "metadata = pd.read_csv('../UrbanSound Dataset sample/metadata/UrbanSound8K.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    class_label = row[\"class_name\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) model architecture \n",
    "\n",
    "\n",
    "We will modify our model to be a Convolutional Neural Network (CNN) again using Keras and a Tensorflow backend. \n",
    "\n",
    "Again we will use a `sequential` model, starting with a simple model architecture, consisting of four `Conv2D` convolution layers, with our final output layer being a `dense` layer. \n",
    "\n",
    "The convolution layers are designed for feature detection. It works by sliding a filter window over the input and performing a matrix multiplication and storing the result in a feature map. This operation is known as a convolution. \n",
    "\n",
    "\n",
    "The `filter` parameter specifies the number of nodes in each layer. Each layer will increase in size from 16, 32, 64 to 128, while the `kernel_size` parameter specifies the size of the kernel window which in this case is 2 resulting in a 2x2 filter matrix. \n",
    "\n",
    "The first layer will receive the input shape of (40, 174, 1) where 40 is the number of MFCC's 174 is the number of frames taking padding into account and the 1 signifying that the audio is mono. \n",
    "\n",
    "The activation function we will be using for our convolutional layers is `ReLU` which is the same as our previous model. We will use a smaller `Dropout` value of 20% on our convolutional layers. \n",
    "\n",
    "Each convolutional layer has an associated pooling layer of `MaxPooling2D` type with the final convolutional layer having a `GlobalAveragePooling2D` type. The pooling layer is do reduce the dimensionality of the model (by reducing the parameters and subsquent computation requirements) which serves to shorten the training time and reduce overfitting. The Max Pooling type takes the maximum size for each window and the Global Average Pooling type takes the average which is suitable for feeding into our `dense` output layer.  \n",
    "\n",
    "Our output layer will have 10 nodes (num_labels) which matches the number of possible classifications. The activation is for our output layer is `softmax`. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "# from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model \n",
    "\n",
    "For compiling our model, we will use the same three parameters as the previous model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 39, 173, 16)       80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 19, 86, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 19, 86, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 18, 85, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 9, 42, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 9, 42, 32)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 41, 64)         8256      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 20, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 20, 64)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 19, 128)        32896     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 9, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 9, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 128)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44602 (174.23 KB)\n",
      "Trainable params: 44602 (174.23 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "55/55 [==============================] - 1s 8ms/step - loss: 8.2040 - accuracy: 0.1065\n",
      "Pre-training accuracy: 10.6468%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Here we will train the model. As training a CNN can take a sigificant amount of time, we will start with a low number of epochs and a low batch size. If we can see from the output that the model is converging, we will increase both numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 3.5760 - accuracy: 0.1741\n",
      "Epoch 1: val_loss improved from inf to 2.06145, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 190ms/step - loss: 3.5760 - accuracy: 0.1741 - val_loss: 2.0615 - val_accuracy: 0.2284\n",
      "Epoch 2/72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tylervu/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 1.8564 - accuracy: 0.3346\n",
      "Epoch 2: val_loss improved from 2.06145 to 1.82104, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 1.8564 - accuracy: 0.3346 - val_loss: 1.8210 - val_accuracy: 0.3709\n",
      "Epoch 3/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.5950 - accuracy: 0.4248\n",
      "Epoch 3: val_loss improved from 1.82104 to 1.58004, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 1.5950 - accuracy: 0.4248 - val_loss: 1.5800 - val_accuracy: 0.4671\n",
      "Epoch 4/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.4288 - accuracy: 0.4905\n",
      "Epoch 4: val_loss improved from 1.58004 to 1.45514, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 192ms/step - loss: 1.4288 - accuracy: 0.4905 - val_loss: 1.4551 - val_accuracy: 0.5031\n",
      "Epoch 5/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.3301 - accuracy: 0.5309\n",
      "Epoch 5: val_loss improved from 1.45514 to 1.37081, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 183ms/step - loss: 1.3301 - accuracy: 0.5309 - val_loss: 1.3708 - val_accuracy: 0.5266\n",
      "Epoch 6/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.2532 - accuracy: 0.5571\n",
      "Epoch 6: val_loss improved from 1.37081 to 1.27913, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 1.2532 - accuracy: 0.5571 - val_loss: 1.2791 - val_accuracy: 0.5730\n",
      "Epoch 7/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.1899 - accuracy: 0.5835\n",
      "Epoch 7: val_loss improved from 1.27913 to 1.22794, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 189ms/step - loss: 1.1899 - accuracy: 0.5835 - val_loss: 1.2279 - val_accuracy: 0.5884\n",
      "Epoch 8/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.1518 - accuracy: 0.5993\n",
      "Epoch 8: val_loss improved from 1.22794 to 1.20379, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 1.1518 - accuracy: 0.5993 - val_loss: 1.2038 - val_accuracy: 0.6050\n",
      "Epoch 9/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.1055 - accuracy: 0.6143\n",
      "Epoch 9: val_loss improved from 1.20379 to 1.14763, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 190ms/step - loss: 1.1055 - accuracy: 0.6143 - val_loss: 1.1476 - val_accuracy: 0.6193\n",
      "Epoch 10/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.0447 - accuracy: 0.6354\n",
      "Epoch 10: val_loss improved from 1.14763 to 1.09567, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 190ms/step - loss: 1.0447 - accuracy: 0.6354 - val_loss: 1.0957 - val_accuracy: 0.6525\n",
      "Epoch 11/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.0117 - accuracy: 0.6472\n",
      "Epoch 11: val_loss improved from 1.09567 to 1.01725, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 187ms/step - loss: 1.0117 - accuracy: 0.6472 - val_loss: 1.0173 - val_accuracy: 0.6766\n",
      "Epoch 12/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.9506 - accuracy: 0.6797\n",
      "Epoch 12: val_loss improved from 1.01725 to 0.98438, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 192ms/step - loss: 0.9506 - accuracy: 0.6797 - val_loss: 0.9844 - val_accuracy: 0.6737\n",
      "Epoch 13/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.9080 - accuracy: 0.6838\n",
      "Epoch 13: val_loss improved from 0.98438 to 0.95645, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 191ms/step - loss: 0.9080 - accuracy: 0.6838 - val_loss: 0.9564 - val_accuracy: 0.6920\n",
      "Epoch 14/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8868 - accuracy: 0.6941\n",
      "Epoch 14: val_loss improved from 0.95645 to 0.92626, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 187ms/step - loss: 0.8868 - accuracy: 0.6941 - val_loss: 0.9263 - val_accuracy: 0.6961\n",
      "Epoch 15/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8497 - accuracy: 0.7099\n",
      "Epoch 15: val_loss improved from 0.92626 to 0.87188, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 206ms/step - loss: 0.8497 - accuracy: 0.7099 - val_loss: 0.8719 - val_accuracy: 0.7149\n",
      "Epoch 16/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8215 - accuracy: 0.7094\n",
      "Epoch 16: val_loss improved from 0.87188 to 0.83796, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 0.8215 - accuracy: 0.7094 - val_loss: 0.8380 - val_accuracy: 0.7293\n",
      "Epoch 17/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7853 - accuracy: 0.7350\n",
      "Epoch 17: val_loss improved from 0.83796 to 0.81491, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 192ms/step - loss: 0.7853 - accuracy: 0.7350 - val_loss: 0.8149 - val_accuracy: 0.7247\n",
      "Epoch 18/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7692 - accuracy: 0.7331\n",
      "Epoch 18: val_loss improved from 0.81491 to 0.80535, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 198ms/step - loss: 0.7692 - accuracy: 0.7331 - val_loss: 0.8053 - val_accuracy: 0.7287\n",
      "Epoch 19/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7446 - accuracy: 0.7479\n",
      "Epoch 19: val_loss improved from 0.80535 to 0.77481, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 197ms/step - loss: 0.7446 - accuracy: 0.7479 - val_loss: 0.7748 - val_accuracy: 0.7464\n",
      "Epoch 20/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7074 - accuracy: 0.7589\n",
      "Epoch 20: val_loss improved from 0.77481 to 0.70793, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 197ms/step - loss: 0.7074 - accuracy: 0.7589 - val_loss: 0.7079 - val_accuracy: 0.7624\n",
      "Epoch 21/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6880 - accuracy: 0.7675\n",
      "Epoch 21: val_loss improved from 0.70793 to 0.69336, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 0.6880 - accuracy: 0.7675 - val_loss: 0.6934 - val_accuracy: 0.7733\n",
      "Epoch 22/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6699 - accuracy: 0.7770\n",
      "Epoch 22: val_loss improved from 0.69336 to 0.65393, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 5s 194ms/step - loss: 0.6699 - accuracy: 0.7770 - val_loss: 0.6539 - val_accuracy: 0.8025\n",
      "Epoch 23/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.7795\n",
      "Epoch 23: val_loss did not improve from 0.65393\n",
      "28/28 [==============================] - 6s 221ms/step - loss: 0.6528 - accuracy: 0.7795 - val_loss: 0.6934 - val_accuracy: 0.7842\n",
      "Epoch 24/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.7707\n",
      "Epoch 24: val_loss improved from 0.65393 to 0.63094, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 231ms/step - loss: 0.6498 - accuracy: 0.7707 - val_loss: 0.6309 - val_accuracy: 0.8100\n",
      "Epoch 25/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6183 - accuracy: 0.7895\n",
      "Epoch 25: val_loss improved from 0.63094 to 0.62125, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 213ms/step - loss: 0.6183 - accuracy: 0.7895 - val_loss: 0.6213 - val_accuracy: 0.8117\n",
      "Epoch 26/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.8037\n",
      "Epoch 26: val_loss improved from 0.62125 to 0.60834, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.5838 - accuracy: 0.8037 - val_loss: 0.6083 - val_accuracy: 0.8065\n",
      "Epoch 27/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5708 - accuracy: 0.8113\n",
      "Epoch 27: val_loss improved from 0.60834 to 0.59795, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 229ms/step - loss: 0.5708 - accuracy: 0.8113 - val_loss: 0.5979 - val_accuracy: 0.8105\n",
      "Epoch 28/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8004\n",
      "Epoch 28: val_loss improved from 0.59795 to 0.57833, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.5762 - accuracy: 0.8004 - val_loss: 0.5783 - val_accuracy: 0.8157\n",
      "Epoch 29/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.8210\n",
      "Epoch 29: val_loss improved from 0.57833 to 0.57065, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 0.5466 - accuracy: 0.8210 - val_loss: 0.5707 - val_accuracy: 0.8180\n",
      "Epoch 30/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.8153\n",
      "Epoch 30: val_loss did not improve from 0.57065\n",
      "28/28 [==============================] - 6s 210ms/step - loss: 0.5446 - accuracy: 0.8153 - val_loss: 0.6025 - val_accuracy: 0.7991\n",
      "Epoch 31/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5193 - accuracy: 0.8232\n",
      "Epoch 31: val_loss improved from 0.57065 to 0.56095, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 0.5193 - accuracy: 0.8232 - val_loss: 0.5610 - val_accuracy: 0.8197\n",
      "Epoch 32/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.8261\n",
      "Epoch 32: val_loss improved from 0.56095 to 0.52887, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.5101 - accuracy: 0.8261 - val_loss: 0.5289 - val_accuracy: 0.8323\n",
      "Epoch 33/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4938 - accuracy: 0.8306\n",
      "Epoch 33: val_loss improved from 0.52887 to 0.49984, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 214ms/step - loss: 0.4938 - accuracy: 0.8306 - val_loss: 0.4998 - val_accuracy: 0.8409\n",
      "Epoch 34/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4910 - accuracy: 0.8319\n",
      "Epoch 34: val_loss did not improve from 0.49984\n",
      "28/28 [==============================] - 6s 210ms/step - loss: 0.4910 - accuracy: 0.8319 - val_loss: 0.5178 - val_accuracy: 0.8386\n",
      "Epoch 35/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.8366\n",
      "Epoch 35: val_loss improved from 0.49984 to 0.45572, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 212ms/step - loss: 0.4755 - accuracy: 0.8366 - val_loss: 0.4557 - val_accuracy: 0.8580\n",
      "Epoch 36/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.8470\n",
      "Epoch 36: val_loss did not improve from 0.45572\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.4541 - accuracy: 0.8470 - val_loss: 0.4945 - val_accuracy: 0.8472\n",
      "Epoch 37/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.8452\n",
      "Epoch 37: val_loss did not improve from 0.45572\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.4481 - accuracy: 0.8452 - val_loss: 0.4824 - val_accuracy: 0.8523\n",
      "Epoch 38/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.8438\n",
      "Epoch 38: val_loss did not improve from 0.45572\n",
      "28/28 [==============================] - 6s 212ms/step - loss: 0.4499 - accuracy: 0.8438 - val_loss: 0.4722 - val_accuracy: 0.8500\n",
      "Epoch 39/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.8515\n",
      "Epoch 39: val_loss did not improve from 0.45572\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.4409 - accuracy: 0.8515 - val_loss: 0.4955 - val_accuracy: 0.8420\n",
      "Epoch 40/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4311 - accuracy: 0.8545\n",
      "Epoch 40: val_loss improved from 0.45572 to 0.43280, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 206ms/step - loss: 0.4311 - accuracy: 0.8545 - val_loss: 0.4328 - val_accuracy: 0.8649\n",
      "Epoch 41/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.8659\n",
      "Epoch 41: val_loss improved from 0.43280 to 0.43130, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.4089 - accuracy: 0.8659 - val_loss: 0.4313 - val_accuracy: 0.8592\n",
      "Epoch 42/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.8674\n",
      "Epoch 42: val_loss did not improve from 0.43130\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 0.3920 - accuracy: 0.8674 - val_loss: 0.4525 - val_accuracy: 0.8575\n",
      "Epoch 43/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.8660\n",
      "Epoch 43: val_loss improved from 0.43130 to 0.42061, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 0.3904 - accuracy: 0.8660 - val_loss: 0.4206 - val_accuracy: 0.8661\n",
      "Epoch 44/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3886 - accuracy: 0.8633\n",
      "Epoch 44: val_loss did not improve from 0.42061\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.3886 - accuracy: 0.8633 - val_loss: 0.4464 - val_accuracy: 0.8609\n",
      "Epoch 45/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3870 - accuracy: 0.8666\n",
      "Epoch 45: val_loss improved from 0.42061 to 0.39293, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 213ms/step - loss: 0.3870 - accuracy: 0.8666 - val_loss: 0.3929 - val_accuracy: 0.8746\n",
      "Epoch 46/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.8756\n",
      "Epoch 46: val_loss did not improve from 0.39293\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.3687 - accuracy: 0.8756 - val_loss: 0.4238 - val_accuracy: 0.8758\n",
      "Epoch 47/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3597 - accuracy: 0.8764\n",
      "Epoch 47: val_loss did not improve from 0.39293\n",
      "28/28 [==============================] - 7s 256ms/step - loss: 0.3597 - accuracy: 0.8764 - val_loss: 0.3994 - val_accuracy: 0.8695\n",
      "Epoch 48/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.8746\n",
      "Epoch 48: val_loss did not improve from 0.39293\n",
      "28/28 [==============================] - 7s 250ms/step - loss: 0.3598 - accuracy: 0.8746 - val_loss: 0.4054 - val_accuracy: 0.8701\n",
      "Epoch 49/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8780\n",
      "Epoch 49: val_loss did not improve from 0.39293\n",
      "28/28 [==============================] - 6s 223ms/step - loss: 0.3476 - accuracy: 0.8780 - val_loss: 0.3960 - val_accuracy: 0.8724\n",
      "Epoch 50/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.8803\n",
      "Epoch 50: val_loss improved from 0.39293 to 0.38835, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.3514 - accuracy: 0.8803 - val_loss: 0.3884 - val_accuracy: 0.8764\n",
      "Epoch 51/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.8829\n",
      "Epoch 51: val_loss did not improve from 0.38835\n",
      "28/28 [==============================] - 5s 195ms/step - loss: 0.3356 - accuracy: 0.8829 - val_loss: 0.4014 - val_accuracy: 0.8752\n",
      "Epoch 52/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3262 - accuracy: 0.8875\n",
      "Epoch 52: val_loss improved from 0.38835 to 0.36135, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 226ms/step - loss: 0.3262 - accuracy: 0.8875 - val_loss: 0.3613 - val_accuracy: 0.8861\n",
      "Epoch 53/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3214 - accuracy: 0.8909\n",
      "Epoch 53: val_loss did not improve from 0.36135\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 0.3214 - accuracy: 0.8909 - val_loss: 0.3814 - val_accuracy: 0.8758\n",
      "Epoch 54/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.8870\n",
      "Epoch 54: val_loss did not improve from 0.36135\n",
      "28/28 [==============================] - 6s 201ms/step - loss: 0.3236 - accuracy: 0.8870 - val_loss: 0.3786 - val_accuracy: 0.8718\n",
      "Epoch 55/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.8945\n",
      "Epoch 55: val_loss did not improve from 0.36135\n",
      "28/28 [==============================] - 6s 213ms/step - loss: 0.3095 - accuracy: 0.8945 - val_loss: 0.3738 - val_accuracy: 0.8809\n",
      "Epoch 56/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.8988\n",
      "Epoch 56: val_loss did not improve from 0.36135\n",
      "28/28 [==============================] - 5s 193ms/step - loss: 0.2973 - accuracy: 0.8988 - val_loss: 0.3806 - val_accuracy: 0.8792\n",
      "Epoch 57/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.8919\n",
      "Epoch 57: val_loss did not improve from 0.36135\n",
      "28/28 [==============================] - 6s 205ms/step - loss: 0.3111 - accuracy: 0.8919 - val_loss: 0.3747 - val_accuracy: 0.8746\n",
      "Epoch 58/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8952\n",
      "Epoch 58: val_loss improved from 0.36135 to 0.35412, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 212ms/step - loss: 0.2963 - accuracy: 0.8952 - val_loss: 0.3541 - val_accuracy: 0.8821\n",
      "Epoch 59/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8966\n",
      "Epoch 59: val_loss did not improve from 0.35412\n",
      "28/28 [==============================] - 6s 222ms/step - loss: 0.2988 - accuracy: 0.8966 - val_loss: 0.3748 - val_accuracy: 0.8827\n",
      "Epoch 60/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.8969\n",
      "Epoch 60: val_loss did not improve from 0.35412\n",
      "28/28 [==============================] - 6s 209ms/step - loss: 0.2969 - accuracy: 0.8969 - val_loss: 0.3826 - val_accuracy: 0.8821\n",
      "Epoch 61/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9128\n",
      "Epoch 61: val_loss did not improve from 0.35412\n",
      "28/28 [==============================] - 6s 215ms/step - loss: 0.2638 - accuracy: 0.9128 - val_loss: 0.3839 - val_accuracy: 0.8792\n",
      "Epoch 62/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.9049\n",
      "Epoch 62: val_loss did not improve from 0.35412\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.2812 - accuracy: 0.9049 - val_loss: 0.3682 - val_accuracy: 0.8901\n",
      "Epoch 63/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.9071\n",
      "Epoch 63: val_loss improved from 0.35412 to 0.34396, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 208ms/step - loss: 0.2716 - accuracy: 0.9071 - val_loss: 0.3440 - val_accuracy: 0.8912\n",
      "Epoch 64/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.9104\n",
      "Epoch 64: val_loss improved from 0.34396 to 0.32270, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 0.2590 - accuracy: 0.9104 - val_loss: 0.3227 - val_accuracy: 0.9010\n",
      "Epoch 65/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2689 - accuracy: 0.9089\n",
      "Epoch 65: val_loss did not improve from 0.32270\n",
      "28/28 [==============================] - 6s 217ms/step - loss: 0.2689 - accuracy: 0.9089 - val_loss: 0.3436 - val_accuracy: 0.8912\n",
      "Epoch 66/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.9091\n",
      "Epoch 66: val_loss did not improve from 0.32270\n",
      "28/28 [==============================] - 7s 246ms/step - loss: 0.2629 - accuracy: 0.9091 - val_loss: 0.3363 - val_accuracy: 0.9004\n",
      "Epoch 67/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9122\n",
      "Epoch 67: val_loss did not improve from 0.32270\n",
      "28/28 [==============================] - 6s 216ms/step - loss: 0.2497 - accuracy: 0.9122 - val_loss: 0.3242 - val_accuracy: 0.9027\n",
      "Epoch 68/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2636 - accuracy: 0.9054\n",
      "Epoch 68: val_loss did not improve from 0.32270\n",
      "28/28 [==============================] - 7s 237ms/step - loss: 0.2636 - accuracy: 0.9054 - val_loss: 0.3666 - val_accuracy: 0.8935\n",
      "Epoch 69/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9078\n",
      "Epoch 69: val_loss did not improve from 0.32270\n",
      "28/28 [==============================] - 6s 211ms/step - loss: 0.2676 - accuracy: 0.9078 - val_loss: 0.3626 - val_accuracy: 0.8895\n",
      "Epoch 70/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9099\n",
      "Epoch 70: val_loss did not improve from 0.32270\n",
      "28/28 [==============================] - 6s 200ms/step - loss: 0.2506 - accuracy: 0.9099 - val_loss: 0.3519 - val_accuracy: 0.8907\n",
      "Epoch 71/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9148\n",
      "Epoch 71: val_loss did not improve from 0.32270\n",
      "28/28 [==============================] - 6s 210ms/step - loss: 0.2472 - accuracy: 0.9148 - val_loss: 0.3538 - val_accuracy: 0.8981\n",
      "Epoch 72/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2379 - accuracy: 0.9181\n",
      "Epoch 72: val_loss improved from 0.32270 to 0.32007, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 6s 205ms/step - loss: 0.2379 - accuracy: 0.9181 - val_loss: 0.3201 - val_accuracy: 0.9033\n",
      "Training completed in time:  0:07:02.098394\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "#num_epochs = 12\n",
    "#num_batch_size = 128\n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model \n",
    "\n",
    "Here we will review the accuracy of the model on both the training and test data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9537580609321594\n",
      "Testing Accuracy:  0.903262734413147\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Training and Testing accuracy scores are both high and an increase on our initial model. Training accuracy has increased by ~6% and Testing accuracy has increased by ~4%. \n",
    "\n",
    "There is a marginal increase in the difference between the Training and Test scores (~6% compared to ~5% previously) though the difference remains low so the model has not suffered from overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions  \n",
    "\n",
    "Here we will modify our previous method for testing the models predictions on a specified audio .wav file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_prediction(file_name):\n",
    "    prediction_feature = extract_features(file_name) \n",
    "    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n",
    "\n",
    "    # Use the model's predict method to generate the probability distribution for each class\n",
    "    predicted_vector = model.predict(prediction_feature)[0]  # Assuming the output is softmax and you want the first prediction\n",
    "\n",
    "    # Get the class index and label\n",
    "    predicted_class = np.argmax(predicted_vector)\n",
    "    predicted_label = le.inverse_transform([predicted_class])\n",
    "    \n",
    "    # Print the predicted class\n",
    "    print(\"The predicted class is:\", predicted_label[0], '\\n')\n",
    "\n",
    "    # Print each class probability\n",
    "    for i, prob in enumerate(predicted_vector):\n",
    "        label = le.inverse_transform([i])\n",
    "        print(label[0], \"\\t\\t : \", f'{prob:.20f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation \n",
    "\n",
    "#### Test with sample data \n",
    "\n",
    "As before we will verify the predictions using a subsection of the sample audio files we explored in the first notebook. We expect the bulk of these to be classified correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "The predicted class is: air_conditioner \n",
      "\n",
      "air_conditioner \t\t :  0.99810034036636352539\n",
      "car_horn \t\t :  0.00000230292471314897\n",
      "children_playing \t\t :  0.00005591498120338656\n",
      "dog_bark \t\t :  0.00001395892286382150\n",
      "drilling \t\t :  0.00137087260372936726\n",
      "engine_idling \t\t :  0.00024184925132431090\n",
      "gun_shot \t\t :  0.00000416626971855294\n",
      "jackhammer \t\t :  0.00008629060903331265\n",
      "siren \t\t :  0.00000028769991899935\n",
      "street_music \t\t :  0.00012391729978844523\n"
     ]
    }
   ],
   "source": [
    "# Class: Air Conditioner\n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/100852-0-0-0.wav' \n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "The predicted class is: drilling \n",
      "\n",
      "air_conditioner \t\t :  0.00010423925414215773\n",
      "car_horn \t\t :  0.00001489401529397583\n",
      "children_playing \t\t :  0.00000129820784877666\n",
      "dog_bark \t\t :  0.00000777781588112703\n",
      "drilling \t\t :  0.99864238500595092773\n",
      "engine_idling \t\t :  0.00000005987637763383\n",
      "gun_shot \t\t :  0.00000020046979898325\n",
      "jackhammer \t\t :  0.00055755622452124953\n",
      "siren \t\t :  0.00000000013417388034\n",
      "street_music \t\t :  0.00067152950214222074\n"
     ]
    }
   ],
   "source": [
    "# Class: Drilling\n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/103199-4-0-0.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "The predicted class is: street_music \n",
      "\n",
      "air_conditioner \t\t :  0.00094828725559636950\n",
      "car_horn \t\t :  0.00009445307659916580\n",
      "children_playing \t\t :  0.07455532997846603394\n",
      "dog_bark \t\t :  0.01619094237685203552\n",
      "drilling \t\t :  0.00007786870992276818\n",
      "engine_idling \t\t :  0.00031345422030426562\n",
      "gun_shot \t\t :  0.00000000133401223401\n",
      "jackhammer \t\t :  0.00001885023084469140\n",
      "siren \t\t :  0.01145354844629764557\n",
      "street_music \t\t :  0.89634722471237182617\n"
     ]
    }
   ],
   "source": [
    "# Class: Street music \n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/101848-9-0-0.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "The predicted class is: car_horn \n",
      "\n",
      "air_conditioner \t\t :  0.00225700438022613525\n",
      "car_horn \t\t :  0.22851300239562988281\n",
      "children_playing \t\t :  0.00165492109954357147\n",
      "dog_bark \t\t :  0.22055572271347045898\n",
      "drilling \t\t :  0.14822559058666229248\n",
      "engine_idling \t\t :  0.00960475206375122070\n",
      "gun_shot \t\t :  0.19518885016441345215\n",
      "jackhammer \t\t :  0.17992477118968963623\n",
      "siren \t\t :  0.01264942344278097153\n",
      "street_music \t\t :  0.00142601772677153349\n"
     ]
    }
   ],
   "source": [
    "# Class: Car Horn \n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/100648-1-0-0.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations \n",
    "\n",
    "We can see that the model performs well. \n",
    "\n",
    "Interestingly, car horn was again incorrectly classifed but this time as drilling - though the per class confidence shows it was a close decision between car horn with 26% confidence and drilling at 34% confidence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other audio\n",
    "\n",
    "Again we will further validate our model using a sample of various copyright free sounds that we not part of either our test or training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n",
      "The predicted class is: dog_bark \n",
      "\n",
      "air_conditioner \t\t :  0.00077685475116595626\n",
      "car_horn \t\t :  0.03165749832987785339\n",
      "children_playing \t\t :  0.00200193352065980434\n",
      "dog_bark \t\t :  0.92742627859115600586\n",
      "drilling \t\t :  0.02914945222437381744\n",
      "engine_idling \t\t :  0.00033214196446351707\n",
      "gun_shot \t\t :  0.00486407661810517311\n",
      "jackhammer \t\t :  0.00058784085558727384\n",
      "siren \t\t :  0.00114176212809979916\n",
      "street_music \t\t :  0.00206223968416452408\n"
     ]
    }
   ],
   "source": [
    "filename = '../Evaluation audio/dog_bark_1.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "The predicted class is: jackhammer \n",
      "\n",
      "air_conditioner \t\t :  0.00455043744295835495\n",
      "car_horn \t\t :  0.00000200521253646002\n",
      "children_playing \t\t :  0.00000581568156121648\n",
      "dog_bark \t\t :  0.00009565481013851240\n",
      "drilling \t\t :  0.00131315516773611307\n",
      "engine_idling \t\t :  0.00285685434937477112\n",
      "gun_shot \t\t :  0.00000353644941242237\n",
      "jackhammer \t\t :  0.99112844467163085938\n",
      "siren \t\t :  0.00004353242184151895\n",
      "street_music \t\t :  0.00000048897226179179\n"
     ]
    }
   ],
   "source": [
    "filename = '../Evaluation audio/drilling_1.wav'\n",
    "\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "The predicted class is: gun_shot \n",
      "\n",
      "air_conditioner \t\t :  0.00000099360102012724\n",
      "car_horn \t\t :  0.00000876784179126844\n",
      "children_playing \t\t :  0.00126167805865406990\n",
      "dog_bark \t\t :  0.00551034463569521904\n",
      "drilling \t\t :  0.00615238584578037262\n",
      "engine_idling \t\t :  0.00013336223491933197\n",
      "gun_shot \t\t :  0.98682749271392822266\n",
      "jackhammer \t\t :  0.00000055469718063250\n",
      "siren \t\t :  0.00000494970390718663\n",
      "street_music \t\t :  0.00009947075886884704\n"
     ]
    }
   ],
   "source": [
    "filename = '../Evaluation audio/gun_shot_1.wav'\n",
    "\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations \n",
    "\n",
    "The performance of our final model is very good and has generalised well, seeming to predict well when tested against new audio data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
